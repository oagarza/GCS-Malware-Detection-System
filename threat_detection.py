import subprocess
import time
import os
from datetime import datetime
from google.cloud import bigquery
from google.cloud import storage
from google.oauth2 import service_account

# Configuration
BIGQUERY_DATASET = 'malware_detection'
BIGQUERY_TABLE = 'detected_events'
GCS_BUCKET_NAME = 'malware-dataset-2024'
INTERFACE = 'eth0'  # Default network interface
PROJECT_ID = 'ransomware-detection-system'
TABLE_ID = f"ransomware-detection-system.malware_detection.detected_events"

# Define the scope
scopes = ["https://www.googleapis.com/auth/bigquery", "https://www.googleapis.com/auth/cloud-platform"]

# Load the credentials and set the scope
credentials = service_account.Credentials.from_service_account_file('/etc/gcp/key.json', scopes=scopes)

# Initialize Google Cloud clients with explicit credentials
bigquery_client = bigquery.Client(credentials=credentials, project=PROJECT_ID)
storage_client = storage.Client(credentials=credentials, project=PROJECT_ID)

def upload_json_to_bigquery(json_filename, table_id):
    """Uploads the given JSON file to BigQuery."""
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
        autodetect=True,
    )
    with open(json_filename, "rb") as source_file:
        job = bigquery_client.load_table_from_file(source_file, table_id, job_config=job_config)
        job.result()  # Wait for the job to complete.

        if job.errors:
            print("Errors:", job.errors)
        else:
            print(f"Loaded data into {table_id}")

def capture_and_upload(interface=INTERFACE, capture_duration=600):
    """Captures packets with Suricata and uploads the output to BigQuery."""
    # Start Suricata capture
    eve_json_filename = "/var/log/suricata/eve.json"
    suricata_command = [
        'suricata',
        '-i', interface,
        '-l', '/tmp',
        '--set', 'outputs.0.eve-log.enabled=yes',
        '--set', 'outputs.0.eve-log.filetype=json',
        '--set', 'outputs.0.eve-log.filename=' + eve_json_filename,
        '--set', 'af-packet=eth0'
    ]

    print("Starting Suricata for 10 minutes...")
    suricata_process = subprocess.Popen(suricata_command)
    
    # Let Suricata run for the capture duration
    time.sleep(capture_duration)
    
    # Stop Suricata
    print("Stopping Suricata...")
    suricata_process.terminate()
    try:
        suricata_process.wait(timeout=10)
    except subprocess.TimeoutExpired:
        suricata_process.kill()
    
    # Wait a moment to ensure file writing has completed
    time.sleep(5)
    
    # Upload to BigQuery
    if os.path.exists(eve_json_filename):
        print("Uploading eve.json to BigQuery...")
        upload_json_to_bigquery(eve_json_filename, TABLE_ID)
    else:
        print("eve.json file not found. Check Suricata capture for issues.")

if __name__ == "__main__":
    capture_and_upload()
